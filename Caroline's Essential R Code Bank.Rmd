---
title: "Essential R Code Notebook"
author: Caroline Fleming
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
# Welcome to Caroline's Essential R Code Notebook! These functions and syntax references are in no particular order, just the code I find I use most and is most useful. Obviously, these are taken from different r projects and no data is provided, so you will have to play around with the variables and/or datasets that are comparable to what I have generated. But this should at least be a good place to start. In the event that a function does not work, google "what package is x function in"? And you may have to install and then library that package. Also recall that you can use ?function() (e.g. ?reshape) to ask R for the documentation on the function.

#INPUTTING PACKAGES and DATA
```{r Install and upload packages commonly used}
#Generally, to install a package that you need you use the function install.packages(), to actually call that package into your working environment you use library()
library("lme4")
library("lmerTest")
library("emmeans")
library("pbkrtest")
library("reshape2") #reshape melts our datasets in the way we wan
library("plyr")
library("tidyverse")
  #tidyverse contains ggplot2, tibble, dplyr, stringr, tibble, and readr
library("Rmisc")
library("performance")
library("see")
library("ggrepel")

#increase the number of results it will show
options(max.print=1000000) #this just prevents R from truncating the number of decimal places it will show you
```

```{r Working directory}
#Establish working directory
require("knitr")
opts_knit$set(root.dir = "C:/Users/carol/Downloads") #I find that when doing R markdowns (which is what I use the most) using knitr to define my working directory works best
#NOTE if you are use R projects, you don't need knitr
getwd() #this will tell you what your current working directory is
```


```{r Upload data / write data to a csv}
#Upload your data from CSV
total_data <- read.csv("C:/Users/carol/Downloads/data.csv", header = TRUE) #this is how you add data. Header=TRUE tells R that there is a header column with data labels.
#or if you are in a project
total_data <- read.csv("data.csv", header=TRUE) #you just dont need the extra working directory because it knows what working directory to pull it from

#Uploading data from excel spreadsheet
data_sockeye <- readxl::read_xls("pnas.1417063112.sd02.xls",na = "NA", sheet=1)

#write data to a csv
write.csv(df,file='/Users/admin/new_file.csv', row.names=TRUE)

```

```{r Connect GIT repro on machine to github}
#if you have a git file already on your computer and want to put everything into a repository, do the following in the TERMINAL:
# move to the project directory
cd Projects/website

# initiate the upstream tracking of the project on the GitHub repo
git remote add origin https://github.com/hansenjohnson/website.git

# pull all files from the GitHub repo (typically just readme, license, gitignore)
git pull origin master #often you can just skip this

# set up GitHub repo to track changes on local machine
git push -u origin master

#now everything should be connected and you can use the R studio git tab as normal
```


#BASIC DATA MANIPULATION

```{r tidyverse 101 - rename() / mutate() / filter() / select()}

#Tidyverse is awesome for data manipulation (specifically dyplr) and I was hesitant to use it for WAY too long. Here is an example of how it works:
seq=(seq(from=1, to=50, by=3))
data <- data.frame(seq=(seq(from=1, to=50, by=3)),num=seq*10)
#this is just a random dataset I made with a seqence of numbers in two columns

#Rename seq to sequence
data %>% #use the pipe operator to add lines of code on the same dataset together
  rename(sequence=seq)
data #now we have changed "seq" to "sequence"

#Mutate to make a new column depending on a condition
wq_all <- wq_all %>% mutate(season = case_when(
    month==1 | month==2 | month==3 ~ "Winter" , #need two equal signs
      month==4 | month==5 | month==6 ~ "Spring" ,
    month==7 | month==8 | month==9 ~ "Summer" ,
    month==10 | month==11 | month==12 ~ "Fall" 
  )
)

# OR  
data_sockeye_seymour %>%
 mutate("data_segment" = case_when(
    yr >= 1948 | yr <= 1976 ~ "1948-1976",
   yr >= 1977 |  yr <= 2005 ~ "1977-2005"  #need two equal signs
   ))

#Subset in tidyverse using filter()
menhaden_flights <- menhaden_all %>%
  filter(Flight.Type!="DMF Validation Flight") %>% #filtering only if it was NOT a validation flight
  filter(Did.Not.Fly==0)


# Change classid to be a factor
school_3_data <-
    school_3_data %>%
    mutate(classid = factor(classid)) #in base R you would use as.factor() but here you use mutate

#select only these columns from our dataset to create a new dataset
wq_menhaden_all_lag_NEW <- wq_menhaden_all_lag %>%
  dplyr::select(year, season_lag, zone, depth, avg_chl, avg_temp, avg_sal, avg_DO, avg_DO_pct, avg_depth, avg_pH, avg_turb, Number.of.Schools, Number.of.Pounds, Pilot.Name, Sea.Conditions, Sky.Conditions)

```


```{r changing column names rename() and removing them with select() }
#renaming columns in tidyverse
data %>% 
  rename(sequence=seq) #new_name = old_name
data

#lets make a new dataset that ONLY has sequence, not num
data_no_num <- data %>%
  select(-num) #the minus sign just means don't include it

#renaming columns in base R if dplyr freaks out
colnames(pam_all.data)[colnames(pam_all.data) == "sym_state"] ="sym_state_full"
colnames(pam_all.data)[colnames(pam_all.data) == "sym_state.1"] ="sym_state"
colnames(pam_all.data)[colnames(pam_all.data) == "temp.1"] ="temp"


```

```{r Subsetting OR selecting}

#These are based off of logical operators, so this will only subset resp_all_nitrate where resp_all_nitrate == 20 is TRUE
all_20C<-subset(resp_all_nitrate.data, resp_all_nitrate.data$temp==20)
#in subsetting the &, aka "and" is an additive function, and |, aka "or" takes anything that has either criteria

#selecting data by row (a condition within  a column)
env_chamber_HOBO_exp_2<- env_chamber_HOBO[env_chamber_HOBO$date  %in% c("8/4/2022", "8/5/2022", "8/6/2022", "8/7/2022"),] #the comma is important at the end here, tells R that we are looking at ROWs not entire columns

#filter (similar to subset)
school_only_entry <- menhaden_all %>% filter(is.na(Number.of.Pounds)==TRUE , Number.of.Schools>=0)
```

```{r pivot}
#flip using pivot so that all of the unique water quality observations are on the top 
wq_pivot <- wq_all %>%
  pivot_wider(names_from=parameter, values_from=measure)


```

```{r group_by and then summarize() a dataset}
#you can use group_by to group the data so that you can perform some functions by batching. Let's say we have our water quality data and we want to group by zone, depth, and date. Then we want to summarize (aka mean or something) those at the SAME zone, depth, and date. Basically it makes little mini subdatasets that you can manipulate
wq_avg_over_zone  <- wq_pivot  %>% 
  group_by(zone, depth, date_formatted) #this is really helpful before you plot something
?group_by
#you can always ungroup if you want to
wq_all_lag_2  <- ungroup(wq_all_lag_2)

?summarise()
#creates a new data frame that has one row for each combination of grouping variables (NEED TO group_by first) this can be really helpful for plotting
wq_all_lag_2 <- wq_all_lag_1 %>%
  group_by(year, zone, depth, season_lag) %>%
  dplyr::summarise(across(all_of(v_avg_wq), #sometimes you have to specify that you want the dyplr function because there are other summarise() functions
                mean,na.rm=T)) #summarizing them by the mean
      

```


```{r joining datasets together and ensure no repeat observations}
#join our water quality and menhaden data by date_formatted and zone
wq_menhaden_all<- left_join(wq_avg_over_zone, menhaden_all, by=join_by(date_formatted, zone))

#makes sure that there are no repeat observations using distinct()
wq_menhaden_all_1<- wq_menhaden_all %>%
  distinct(date_formatted, zone, depth, .keep_all = T) # .keep_all=T just keeps all the other columns
```

# CODING BUILDING BLOCKS: FOR, WHILE, IF/ELSE, FUNCTIONS
```{r for loops}
#general recipe
for(ITERATOR in VALUE_SET){
  YOUR_CODE_HERE
}

Population_size <- 0
Constant_growth <- 2

for(ITERATOR in 1:10){ #go through this loop 10 times, printing the output every time
  Population_size = Population_size + Constant_growth
  print(Population_size)
}

#now initialize using an empty vector
vector_pop_size <- vector()
pop_size <- 0
constant_growth <- 2
for (ITERATOR in 1:10) {
  pop_size <- pop_size + constant_growth
  vector_pop_size[ITERATOR] <- pop_size
}
#Iterator in this case tells us WHERE in the resulting vector to put to solution to the for loop


#More complicated example
N_ <- 5000 #initialize the code with input values
m_ <- 500
K_ <- 500

v_N_estimate <- vector(length=100) #create an empty vector for it to put data into
?sample

for(iter_i in 1:length(v_N_estimate)){
  
  fish_vector <- vector(length=N_) #create an empty vector
  
  individuals_1 <- sample(N_,m_) #grab a random sample of 1:N_ (1:5000) of  length m_ (500)
  fish_vector[individuals_1] <- 1 #put the value of 1 into the individuals_1 position in fish_vector
  
  individuals_2 <- sample(N_,K_) #grab a new random sample of 1:N_ (1:5000) of  length m_ (500)
  k_i <- sum(fish_vector[individuals_2]) #take the sum of fish_vector at the indices of individuals_2
  
  v_N_estimate[iter_i] <- m_*K_/k_i
}
```

```{r while() loop}


```

```{r functions}
#generally what we want to do is store a variable with a set of instructions
NAME_FOR_FUNCTION <- function(INPUTS){
  CALCULATIONS
  return(OUTPUT)
}

f_demo_3_input <- function(x,m,b){ #every time you call the function you need to input x, m and b
  y = m*x+b
  return(y)
}

f_demo_3_input(x=2, m=2, b=7) #now we have a solution to these instructions

#more complicated function
do_1_mark_recapture <- function(total_population,catch_sample_1,catch_sample_2){
  fish_vector <- vector(length=total_population)
  
  individuals_1 <- sample(total_population,catch_sample_1)
  fish_vector[individuals_1] <- 1
  
  individuals_2 <- sample(total_population,catch_sample_2)
  number_recatch <- sum(fish_vector[individuals_2])
  
  return(number_recatch)
}

#or 

#Code to define your function that calculates dN/dt
dn_dt_function_logistic <- function(N_i_ , r_ ,K_, E_){
  dn_dt <- (r_ * N_i_*(1-(N_i_ / K_))) - (E_*N_i_)} #this calculates dn/dt at a particular population level
results <- dn_dt_function_logistic(N_i_=1, r_=1 ,K_=2, E_=0) #so that does work


```
```{r While loops}
#It is similar to a for loop, but with a conditional statement
#This is helpful when we want to know HOW many interations it take to complete some task
while(CONDITION){
  CODE
}

#simple example:
counter <- 1 #initialize the while loop
while(counter < 10){ #go until you hit 9
  print(counter)
  counter = counter + 1
}

#or 

#how many cell divisions it takes to over 10 million cells
N_cells <- 1 #initialize
counter <- 1 #counting the generations, but need to initialize first
while(N_cells < 10000000){
 N_cells <- N_cells*2 #(or N_cells + N_cells)
  #population change
 counter <- counter + 1
}
print(counter) #took 25 generations

#or 
N_fish_t <- .5
dt <- 0.05
counter <- 0
while(counter < 40){
  counter <- counter + 1
  N_fish_t <- N_fish_t + (N_fish_t * (1 - N_fish_t))*dt
  print(N_fish_t)
}
```

```{r if / else }
#generally the guts of these work in Boolean true / false
if(CONDITION){
  CODE_1
}else{
  CODE_2
}


#simple example
a <- 1
b <- 2
if(a < b){
  print("Yes!")
}else{
  print("No!")
}

```


# STATISTICAL TESTS AND MODELING

```{r Testing for normality and package to suggest transformation}
# In order to do a lot of statistical tests, you want your data to be normal. You can test this visually by looking at Q-Q plots and histograms. You should do this for each of your dependent variables.
#Histogram
hist(total_data$WEIGHT_M0) #You want this to look like a normal distribution bell curve
#Q-Q plot
qqnorm(total_data$WEIGHT_M0)#you want the points to follow the line pretty cleanly

#You can also test if your data is normal using a Shapiro test
shapiro.test(resp_all.data$resp_rate) 


#The below chunk of code takes a column of your data, checks to see if it is normal, and if it is not normal it suggests a certain transformation it thinks will make the data normal, you need the package bestNormalize
#install new package to see what the best transformation is
#install.packages("bestNormalize")
library("bestNormalize")
bestNormalize(resp_all_nitrate.data$resp_rate)
#in order for this to work I need to change the PAM_delta in to a vector
resp_all_nitrate.data$resp_rate_vector<-as.vector(resp_all_nitrate.data$resp_rate_vector)
#then I run the orderNorm command into a new object
orderNormobj_resp_rate<-orderNorm(resp_all_nitrate.data$resp_rate)
#this object gives us what we want, but we have to index from that object and put it in a new column
resp_all_nitrate.data$ordernorm_resp_rate<-orderNormobj_resp_rate$x.t

#Let say that you want to add another column with the transformed data
#adding a log transformed resp rate
resp_all_nitrate.data$log_resp_rate<-log(abs(resp_all_nitrate.data$resp_rate))
```

```{r Checking if there are outliers in the dataset}
#If you think there might be a statistical outlier in your dataset, you can do a grubbs test, which will tell you if there is an outlier and the value of that outlier
#This function is in the outliers package
#install.packages("outliers")
#library(outliers)
grubbs.test(resp_all_nitrate.data$resp_rate) 
```

```{r T tests parametric and nonparametric}
#When your data follows a normal distribution
t.test(data$PAM , data$N_species_dose, paired=TRUE) #can have paired=TRUE or paired=FALSE depending on your data

#Use this Wilcoxon test when the data does not follow a normal distribution -- worth noting that Ethan Deyle really likes these tests!
wilcox.test(data$PAM , data$N_species_dose, paired=TRUE) #can have paired=TRUE or paired=FALSE depending on your data
```

```{r ANOVA and Tukey Posthocs}
#Do an ANOVA when you have multiple independent variables that you think explain your dependent variable
aov_resp_e_coli <- aov(resp_rate ~ temp + food, data=e_coli)
summary(aov_resp_e_coli) # you have to include this summary function here to actually look at the results of the ANOVA
#Now you will likely want to do a Tukey's pairwise posthoc
TukeyHSD(aov_resp_e_coli, "temp") #this tells you that you want to see pairwise comparisons of temperature
```

```{r Intro to linear modeling and linear trendlines}
#parts of a regression:
lm(y ~ x, data=mydata)
#multiple regression includes MULTIPLE linear response variables, each with their own predictor and slope
#What you need to run the model / some important notes
  #independence of PREDICTOR variables
  #assumes linearity (normality)
  #interactions may be important
  #convert all predictors to factors
  #you may need to scale parameters (e.g. make everything in the metric system)

#The sign of a linear regression coefficient tells you whether there is a positive or negative correlation between each independent variable and the dependent variable
#Rs default option is to give you the difference based on the reference class of each predictor variable
#Reference grup is always the first group in a factor unless you hcange the setting


#adding a linear trendline
reg_model<-kn(response ~ predictor, data=reg_demo)
summary(reg_model)
reg_model
reg_coef_plot <- tidy(reg_model) #extract the coefficients from the reg model 
#now we can plot the trendline
ggplot(reg_model, aes(x=predictor, y=response)) +
  geom_point() + 
  theme_minimal () +
  geom_abline(intercept = reg_model$estimate[1],
                  slope = reg_model$estimate[2])

#A single intercept is the global mean of the data (null model)
# Use a linear model to estimate the global intercept
lm(mathgain ~ 1, data = school_3_data)
#this will give you the same answer as
school_3_data %>%
    summarize(mean(mathgain)) #aka, just the mean

# Building a mutliple regression
lm(mathgain ~ mathkind + classid - 1, data=school_3_data) #this -1 allows us to look at the coefficient for each classoom
#order DOES matter here

# you can graph predictions from the model as lines
# Re-run the models to load their outputs
lm_out <- lm(mathgain ~ classid + mathkind, data = student_data)
lmer_out <- lmer(mathgain ~ mathkind + (1 | classid), data = student_data)

# Add the predictions to the original data
student_data_subset <-
    student_data %>%
    mutate(lm_predict = predict(lm_out),
           lmer_predict = predict(lmer_out)) %>%
    filter(schoolid == "1")

# Plot the predicted values
ggplot(student_data_subset,
       aes(x = mathkind, y = mathgain, color = classid)) +
    geom_point() +
    geom_line(aes(x = mathkind, y = lm_predict)) +
    geom_line(aes(x = mathkind, y = lmer_predict), linetype = 'dashed') +
    xlab("Kindergarten math score") +
    ylab("Math gain later in school") +
    theme_bw() +
    scale_color_manual("Class ID", values = c("red", "blue"))



```

```{r Basic linear models -  lm() }
#When you have a complicated dataset and you want to see how various different independent variables effect your dependent variable and the magnitude of that effect, you usually want to create a linear model. This is essentially the same math as an ANOVA (from what Caroline understands, but she's no stats buff), but is really the gold standard for investigating the effects of your variables on your output.

#When generating a linear model, you first have to decide whether or not you want to include random effects. In a simple linear model using the function lm(), you are essentially doing an ANOVA. Often, though, we have random effects such as the tank or jar the organism was in, the colony the coral came from, or something else that we want to control for so that any variation caused by that random effect is put to the side and we can see what actually infludenced the data. When you have a random effect in your data, you want a linear model with random effects, which uses the lmer() function from the lme4 package.

#The lm() and lmer() functions work similarly, though in their syntax -- first you have your dependent variable, then a tilde (~), then your independent variables added to gether using a + sign, then a data= at the end. In the lmer() function, you include a random effect which for some weird reason needs to look like this,"(1|col_num)". Here is the same model, with and without a random effect:

simple_model <- lm(resp_rate ~ e_coli_plate + temperature, data=resp_all.data)

# NOTE: there is code that can give you a nice plot of linear model including residuals and standard error:
termplot(lm_mod, partial.resid=TRUE, se=TRUE)
```


```{r Linear models with random effects - LMER }

random_effects_model <- lmer(resp_rate ~ e_coli_plate + temperature + (1|tank_number), data=resp_all.data) #adding this random effect of tank number will allow me to investigate whether the data was influenced by the tank the corals were in
#using the (1|tank_number) syntax allows the intercept to vary between levels of tank_number
#There are several different ways to include a random effect in your linear model based on what you want:
(1 | group) # random intercept with fixed mean
(1 | g1/g2) # intercepts vary among g1 and g2 within g1
(1 | g1) + (1 | g2) # random intercepts for 2 variables
x + (x | g) # correlated random slope and intercept
x + (x || g) #uncorrelated random slope and intercept

# It is really important to know that in order to look at the results of your model, you need to use the summary() function around the object that you named the model.
summary(simple_model)
summary(random_effects_model)


#Here's how to interpret the output (YIIIIKES, this is tricky, but stick with me)
#Example model:
resp_random_model <- lmer(resp_rate ~ N_species_dose + food +(1|vial_ID), data=resp_all_nitrate.data)
summary(resp_random_model)
fixedf(resp_random_model) #gives us the fixed - effects estimates
confint(resp_random_model) # gives us the confidence intervals
#Example output as it would appear in your console:
# Linear mixed model fit by REML. t-tests use Satterthwaite's method ['lmerModLmerTest']
# Formula: resp_rate ~ N_species_dose + food + (1 | vial_ID)
#    Data: resp_all_nitrate.data
# 
# REML criterion at convergence: -3314.1
# 
# Scaled residuals: 
#      Min       1Q   Median       3Q      Max 
# -2.98495 -0.55448  0.09901  0.74610  2.13819 
# 
# Random effects:
#  Groups   Name        Variance    Std.Dev.
#  vial_ID  (Intercept) 0.000000000 0.000000
#  Residual             0.000002937 0.001714
# Number of obs: 342, groups:  vial_ID, 22
# 
# Fixed effects:
#                         Estimate   Std. Error           df t value            Pr(>|t|)    
# (Intercept)          -0.00426781   0.00019997 337.00000000 -21.342 <0.0000000000000002 ***
# N_species_doseN_1.5   0.00036093   0.00025201 337.00000000   1.432               0.153    
# N_species_doseN_5    -0.00001925   0.00025797 337.00000000  -0.075               0.941    
# N_species_doseN_18   -0.00033715   0.00026798 337.00000000  -1.258               0.209    
# foodS                -0.00007661   0.00018536 337.00000000  -0.413               0.680    
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# 
# Correlation of Fixed Effects:
#             (Intr) N__N_1. N__N_5 N__N_18
# N_spc_N_1.5 -0.629                       
# N_spcs_dN_5 -0.610  0.486                
# N_spcs_N_18 -0.590  0.468   0.457        
# foodS       -0.458  0.004  -0.004  0.001 
# optimizer (nloptwrap) convergence code: 0 (OK)



#What the heck does any of this mean?! Well really you want to focus on the Fixed effects table, specifically the Pr(>|t|) and the Estimate.

#The take-home that I get from this model is that either N_species_dose or food explain much about the model, because the p values for those are not significant (they are greater than 0.05).

#An estimate that is negative indicates that as that value increases, the response variable decreases. For example here, N species doses of 5 and 8 seem to increase respiration rate

#Honestly, when I am looking at these tables I am much more interested in pairwise comparisons between different independent variables (which we will get to in a section or two) then the effects on the whole model. But essentially this table tells you what independent variables are influencing changes in the dependent variable, and to what extent (estimate).

#The intercept is a really confusing concept that I still havent even fully understood, but what I do know is that 1) we don't really care about the intercept and 2) whatever independent variable you see missing is looped into that intercept. For example, in this dataset we had to different types of food regimes, fed (F) or starved (S). In the Fixed Effects table, I see foodS but not foodF. That is because foodF is the reference class, and so the model is comparing the effect of foodF (which we cannot see because it is magically inside the intercept) to foodS which we can see, and the p value for that comparison is 0.68 (not significant). Also, I dont see our control dosage of 0 for N_species_dose in the fixed effects table. That means it's wrapped up in the intercept.

#Effect size can be interesting to look into, but it's more helpful to look into when there are more straight forward pairwise interactions in our posthocs (more on that in a bit)

#A negative correlation number tells us if we were to do this experiment again, we would expect a negative relationship between our fixed effects and intercept

# NOTE: there is code that can give you a nice plot of linear model including residuals and standard error:
termplot(lm_mod, partial.resid=TRUE, se=TRUE)

# LMER models with random-effect slopes
# you may want to use a random-effect slope if you have reason to believe some factor will influence several things-- If we are looking at state birth data, we can estimate random-effect slopes for each state like this (similar to what we did above):
# Include the AverageAgeofMother as fixed-effect and State as a random-effect
model_a <- lmer(BirthRate ~ AverageAgeofMother + (1 | State), county_births_data)
tidy(model_a)
#random-effect slope may be estimated for each group using (slope | group) syntax with lmer()
#but lets say now we want to estimate a random-effect slope of each state, for example, perhaps the log of total population of each county, LogTotalPop, changes the birth rate of a county AND varies by state. AKA, we think that the log total pop will change WITHIN each state
model_b <- lmer(BirthRate ~ AverageAgeofMother + (LogTotalPop | State), county_births_data)
tidy(model_b)
# we could also look at the age as a random effect within EACH STATE
model_c <- lmer(BirthRate ~ AverageAgeofMother + (AverageAgeofMother | State), county_births_data)
summary(model_c)



#NOTE ON SINGULARITY:
#is singluar warning tells us that the model is not quite right
```


```{r Linear model selection for normally distributed data}
# STEPWISE LINEAR MODEL SELECTION
# We want to do linear model selection because we want to see what independent variables actually explain changes in dependent variables, and we don't care so much about something that is not explaining our data.  By this, I mean interesting comes out as significant in the overall model, and not interesting is not significant in the overall model. There is actually a cool set of mathematical functions that can actually pull out what is interesting and take away what is not.


#Let's complete a linear model selection for simple linear models with NO random effects. We have to include every possible factor and the combined effects of every factor, the * indicates that those effects have been combined

#stepAIC is from the MASS package
fullModel = lm(resp_rate ~   N_species_dose + temp + food + sym_state + e_coli_plate + N_species_dose*temp + N_species_dose*food + N_species_dose*sym_state + N_species_dose*e_coli_plate + temp*food + temp*sym_state + temp*e_coli_plate + food*sym_state + food*e_coli_plate + sym_state*e_coli_plate, data= resp_all_nitrate.data) # model with all 9 variables and possible combinations

nullModel = lm(resp_rate ~  1, data= resp_all_nitrate.data) # model with the intercept only

summary(stepAIC(nullModel, # start with a model containing no variables
                direction = 'forward', # run forward selection
                scope = list(upper = fullModel, # the maximum to consider is a model with all variables
                             lower = nullModel), # the minimum to consider is a model with no variables
                trace = 1)) # do not show the step-by-step process of model selection


#LINEAR MODEL WITH RANDOM EFFECTS
#Now let's do a linear model selection on a linear model with random effects (that was generated using the lmer function from lme4 package)
# This stepwise function includes random effects
full.model <- lmer(resp_rate ~  N_species_dose + temp + food + sym_state + e_coli_plate + N_species_dose*temp + N_species_dose*food + N_species_dose*sym_state + N_species_dose*e_coli_plate + temp*food + temp*sym_state + temp*e_coli_plate + food*sym_state + food*e_coli_plate + sym_state*e_coli_plate +  (1|col_num) + (1|exp_num) , data= resp_all_nitrate.data)
print(step.model<- step(full.model))


#If you want to do this yourself or compare specific models to each other, you want to use an AIC score, which is Aike's Information Criterion, named after some white dude I'm sure. the lower the AIC score, the better or more "fitted" your model is.
#AICing for model selection
AIC(mm_resp_2, mm_resp_3) #A lower AIC indicates a model that explains more of the data, AKA, LOWER AIC= good!
#Generating an AIC table if you want to compare a bunch at once
AIC_table_resp<-AIC(mm_resp_1, mm_resp_2, mm_resp_3, mm_resp_3_a, mm_resp_4, mm_resp_5, mm_resp_6)
AIC_table_resp

```

```{r Checking for model fitness and visualizing model}
#After we generate our "winning" model with the lowest AIC score, we also need to do some checks that our model isn't breaking some serious rules, most of which I dont understand but I follow blindly lol

#Here's an example of me doing that:
#  CHECKING MODEL FITNESS AND VISUALIZING MODEL for our winning model mm_resp_2
#checking for model fit
plot(mm_resp_2) #homogeneity of variance,looks okay--we want them to be relatively distributed around 0
r2(mm_resp_2) #r2 tells us how much variance in our data is explained by our model. Marginal id variance is the amount of variance explaiend just by fixed factors (independent variables), and conditional includes variance explained by random factor. You can interpret these as percentages.
check_collinearity(mm_resp_2) #check for predictor variables are correlated #Low correllation
check_outliers(mm_resp_2) #If there is an outlier, you need to take care of this-- see a chunk of code above

check_heteroscedasticity(mm_resp_2) #heteroscadasticity detected
check_homogeneity(mm_resp_2) #all good, there is homogeneity
check_autocorrelation(mm_resp_2) #autocorrelation detected
#Now this command will plot all of our model diagnostics to visualize them
plot_model(mm_resp_2, type="diag") #everything looks pretty good

#checking out the contribution of each variable to the mode
summary(mm_resp_2)$coef
#we can just interpret t as an index of how important the effect is without trying to use some p-value to describe it

#looking at just the random effect part of the model
VarCorr(mm_resp_2) #not really sure how to interpret this

plot_model(mm_resp_2, "eff") # plots the predicted effects of each variable on respiration rate 
plot_model(mm_resp_2, type="re") # plots random effects 

#let's try using the package relaimpo to give us a sense of the relative contribution of each variable to the model
# link for info on this package: https://advstats.psychstat.org/book/mregression/importance.php
library(relaimpo)
lm_resp_2_relimp <- lm(resp_rate ~ factor(sym_state) + factor(temp) + 
    factor(e_coli_plate), data = resp_all_nitrate.data) #for some reason this code would not run unless I treated them as factors
calc.relimp(lm_resp_2_relimp, type="lmg", rela=TRUE)
#need to identify exactly what lmg means

# Some other notes....
#These coefficient estimates include uncertainty and 95% confidence interval (CI) captures this uncertainty. If a parameter's 95% confidence interval does not include zero, then the parameter is likely statistically different from zero.
# Extract coefficents
lmer_coef <-
    tidy(lmer_classroom, conf.int = TRUE)
# Print coefficents
print(lmer_coef)
# Plot results
lmer_coef %>%
    filter(effect == "fixed" & term != "(Intercept)") %>%
    ggplot(., aes(x = term, y = estimate,
                  ymin = conf.low, ymax = conf.high)) +
    geom_hline(yintercept = 0, color = 'red') + 
    geom_point() +
    geom_linerange() +
    coord_flip() +
    theme_bw() +
    ylab("Coefficient estimate and 95% CI") +
    xlab("Regression coefficient")

```


```{r Tukey posthocs for linear models}
#Posthocs of emmeans package

# Question: is there a dose of nitrate where we see a statistically significant effect on respiration?
#Pairwise for N_species_dose
emm_resp_2_N_species_dose=emmeans(mm_resp_2, specs=pairwise ~ N_species_dose)
summary(emm_resp_2_N_species_dose)
 # contrast          estimate       SE  df t.ratio p.value
 # CNTRL_0 - N_1.5 -0.0004036 0.000225 287  -1.795  0.2777
 # CNTRL_0 - N_18   0.0003303 0.000244 304   1.352  0.5308
 # CNTRL_0 - N_5    0.0000634 0.000233 300   0.272  0.9930
 # N_1.5 - N_18     0.0007339 0.000243 296   3.026  0.0143 ***
 # N_1.5 - N_5      0.0004670 0.000232 291   2.016  0.1843
 # N_18 - N_5      -0.0002669 0.000246 290  -1.086  0.6981

#What does this all mean?

#The p value is pretty straight forward, it is a Tukey's pairwise comparison checking the difference between those two treatments
#The estimate is a bit more interesting and goes to our effect size. The effect that I am looking at is respiration rate, so the effect size is a change in respiration rate. How I would read this, looking specifically at the N_1.5 - N_18 row, is that corals that experienced the N_18 treatment respired 0.0007339 umols more than corals under the N_1.5 treatment. The pattern that I have found is that if the estimate is negative, then the first treatment had a higher value for respiration rate, an dif the estimate is positive, then the second treatment had a higher value for respiration rate.

```

```{r Generalized additive models (GAM)}
# Load mgcv
library(mgcv)

# Fit the model
gam_mod <- gam(accel ~ s(times), data = mcycle)
#s is the smoothing function, we want the x axis to be smoothed here, times that the crash dummy hit its head
#for my environmental data I would want the smoothed data to be season 
?gam()

# Plot the results
plot(gam_mod, residuals = TRUE, pch = 1)
#pch is the plot character to use

# Fit a GAM with 3 basis functions
gam_mod_k3 <- gam(accel ~ s(times, k = 3), data = mcycle)
#k gives you the number of basis functions

# Fix the smoothing parameter at 0.1
gam_mod_s1 <- gam(accel ~ s(times), data = mcycle, sp = 0.1)
#sp is the smoothing parameter

# Fit the GAM
gam_mod_sk <- gam(accel ~ s(times, k=50), sp=0.0001, data = mcycle)
#Visualize the model
plot(gam_mod_sk, residuals = TRUE, pch = 1)
#this is REALLY wiggly 

#if you want to add linear models or categorical data you just add + with no s function like such:
model3 <- gam(hw.mpg ~ s(weight) + fuel, data= mpg, method= "REML")
#store categorical variables as factors

#using the by function you can tell R to calculate a different smooth for each category, below R will calculate a different smooth for diesel vs. gas fuel types
model4 <- gam(hw.mpg ~ s(weight, by=fuel), data= mpg, method= "REML")


# Fit a model to predict city fuel efficiency (city.mpg) with smooth terms of weight, length, and price, but make each of these smooth terms depend on the drive categorical variable using by= in the smooth terms. Include a separate linear term for the drive variable.
mod_city3 <- gam(city.mpg ~ s(weight, by= drive) + s(length, by=drive) + s(price, by=drive), + drive, data = mpg, method = "REML")
#by=drive allows it to fit a smooth for every kind of drive category

# INTERPRET RESULTS FROM GAM
summary(mod_city3)
# Family: what distribution it is assuming
# Gaussian = normal
# Link: identity means it did not transform anything
# Parametric coefficients are the linear components
# Approximate significance of smooth terms covers the smooth terms
# edf= effective degrees of freedom, tells us the complexity of the smooth 
# IMPORTANT: an EDF of 1 is a straight line (linear)
# P values are approximate
# A significant p value is one where you cannot draw a horizontal line through the 95% confidence interval
coef(mod_city3) #can tell you the number of basis functions based on the number of coefficients


# VISUALIZING OUR GAM MODEL
plot(mod) #gives us the regular plot
#partial effects plots show the contribution of each smooth or linear effect of the model, which add up to the overall predictions
plot(mod, shade=TRUE, shade.col="hotpink") #shows us the confidence intervals in shade rather than dotted, and you can adjust the color using shade.col
plot(mod, residuals=TRUE) #adds the residuals in dots around the plot
plot(mod, residuals=TRUE, pch=1, cex=1) #changes the size of the residuals
plot(mod, select=3) #select only the price partial effect, which is term #3 in the model
plot(mod, pages=1, all.terms=TRUE) #plot all the effects in one page (YOU PROBABLY JUST WANT TO USE THIS)
plot(mod, select = 1, shift = coef(mod)[1], seWithMean = TRUE) #this shifts the vale of the intercept (which just allows it to be in terms of the scale we are interested in, it just adds a constant so that the numbers are interpretable) and includes uncertainty of the model with the seWithMean function, generally we want seWithMean=TRUE

#CHECKING THE MODEL FOR FITTING
gam.check(mod)
#first it checks for convergence, if the model has not converged its really not a good model, usually happens when there are too many parameters in the model for not enough data
#basis checking results shows a statistical test for patterns in residuals, which should be random
#each basis check shows the k value (number of basis functions) and p value. If p value is SIGNIFICANT, then residuals are NOT randomly distributed, which means that there are not enough basis functions
#we want the p values here to NOT be significant
#generates 4 plots:
  #we want q-q plot that is close to a straight line
  #we want histogram to be bell shape
  #residuals vs. linear predictions should be randomly distributed around 0
  #response vs. fitted values should clister around the 1:1 linear up line
#if one of the p values is significant, just play with the k= to figure out a better fit and keep using gam.check() until all of the p values under basis dimensions are not significant

#CHECKING CONCURVITY
#In linear models, colinearity is when two variables are so highly correlated that the model could fit to either one of them, and the model fit gets messed up
#Concurvity is when one variable looks like the smooth curve of another, WE DONT WANT THIS
concurvity(mod, full=TRUE) #full=TRUE reports concurvity for each smooth, aka how much each smooth is determined by all the other smooths
#if "worst" if over 0.8, inspect your model more carefully
concurvity(mod, full=FALSE) #full=FALSE returns matrices of pairwise concurvities (you usually want to do this AFTER you do full=TRUE), they pretty much just show how each variable is predetermined by each other variable so you can see which variables are the most closely related
#NOTE: the lower the concurvity, the lower the similarity to the other variables (we really want them to be different so the model can do what it needs to do)

# TWO DIMENSIONAL / MULTIDIMENTIONAL SMOOTHS
mod2d <-gam(y ~ s(x, y), data=dat, method="REML") #models the interaction of the two variables, you can always add different linear or nonlinear components withing s() functions or  + linear term
#summery outputs: 
#gives you single smooth output for the combo of them 
#this takes more effective degrees of freedom and therefore more data
#use this when you have x and y coordinates, generally a natural way to model spatial data
plot(mod2d) # this gives us a contour plot over space from x and y (topographic map), with the dark lines being what the model thinks and the dotted being one standard error higher or lower
plot(mod2d, scheme=1)#3D
plot(mod2d, scheme=2) #heat map 
vis.gam(mod2d, #GAM object
        view=c("x1","x2"), #variables
        plot.type="persp", #kind of plot, 3D perspective plot
        se=2, #displays confidence interval, to plot high and low surfaces
        theta=220, phi=55, r=0.1) #controls the rotation and zoom
vis.gam(mod2d, #GAM object
        view=c("x1","x2"), #variables
        plot.type="contour", #kind of plot, heat map
        too.far=0.01, #too.far indicates what should NOT be plotted because they are too far from the data (too far to extrapolate), scaled from 0-1 on the range of the variables. The white boxes are the areas NOT supported by data, where the model is just not as good, basically you "extrapolate out from the data" some percentage
        color="gray", contour.col="blue", #options for color background and lines
        nlevels=20) #number of contour lines

#FACTOR SMOOTHS-- categorical data
#good for controlling for categories that are not our main category of interest (like if there are too many categories, or one category that has poor data quality is swinging your whole model)
model4c <- gam(hw.mpg ~ s(weight, fuel, bs="fs"), data=mpg, method= "REML")
summary(model4c) # we get one overall term, so we cant look at what specific categories are doing
plot(model4c) #makes a normal plot, but this wont tell us the whole picture. We want to do:
vis.gam(model4c, theta=125, plot.type="persp")

#TENSOR DATA -- used for data on two different units of meaure (aka space or time)
#a tensor is simlar to a smoothing parameter, but it is different in that it has two smoothing parameters, one for each variable
#when you dont want the model to think that two variables are going to vary similarly (like space and time) you want to use a tensor
#each variable gets its own smoothing when you are putting them in an interaction term
gam(y ~ te(x1, x2, k=c(10,20)), data=data, method="REML") #te indicates a tensor, and you can specity a different k for each tensor
#tensor interactions-- you want to use these when you wan to separate out the univariate effect of a variable AND its interaction term 
gam(y ~ s(x1) + s(x2) + ti(x1,x2), data=data, method="REML") #ti allows you to ONLY look at the interaction, and not their independent effects (the smoothing functions take care of their independent effects)
#these models need a lot of data -- I LIKE THIS I FEEL LIKE YOU WANT TO USE IT

#LOGISTIC GAMS FOR BINARY OPERATORS
#presence or absence of organisms
#when you model a binary operator, your prediction is always between 0 and 1
#logistic function does this -- converts binary to predictions
log_gam<-gam(y ~ xt + s(x2), data=dat, family=binomial, method="REML") #the family=binomial is the important piece here
summary(log_gam) #NOTE:outputs are on the log-odds scale, they need to be converted using the logistic function
plogis(0.733) #this number was our estimate, we can use plogis to turn it into a percentage or 67% baseline chance of a positive outcome
#the estimate tells us the probailiyt of a person making a home purchase who has mean values of all variables
# Fit a logistic model
plot(binom_mod) #this is on the log odds scale, so we want to change the scale
plot(binom_mod, pages= 1, trans= plogis) #changes everything to probability
plot(binom_mod, pages= 1, trans= plogis, shift=coef(binom_mod)[1]) #incorporates the model intercept, adds same value to everything
plot(binom_mod, pages= 1, trans= plogis, shift=coef(binom_mod)[1], seWithMean=TRUE) #generally you want to include the SE too, and you can use shade.con to change color etc
#you can tell what has the biggest effect on the outcome variable based on which partial effect spans the largest range on the y axis, so it has the highest probability

#MAKING PREDICTIONS IN GAM
predict(log_mod2, type="link") #generally gams are fit to the link scale which is a normal distribution im pretty sure I have NO idea why they dont just say normal
#for the probability scale, you want type="response"
predict(log_mod2, type="response", se.fit=TRUE) #se.fit=TRUE gives us the standard errors
predict(log_mod2, type="terms")
# Calculate high and low predictions intervals
high_pred <- predictions$fit + 2*predictions$se.fit
low_pred <- predictions$fit - 2*predictions$se.fit
        
# ONE EXAMPLE WITH MY DATA
require("mgcv")
pounds_school_gam <- gam(Number.of.Pounds ~ s(Number.of.Schools), data = menhaden_all, method="REML")
summary(pounds_school_gam)
plot(pounds_school_gam, scale = 0)
layout(1)
gam.check(pounds_school_gam)
#interpreting results from summary() function and plot() graph
#When interpreting GAM results, we want to see when the confidence interval (the dotted bands) departs from 0. For this one, it looks like there is a positive relationship between number of schools and biomass under 225 schools
#GAMs can also "explode" at the end, meaning that the confidence interval is really big
#WHAT WE ARE LOOKING FOR IN THE DIAGNOSTIC PLOTS
# We want the response v fitted to be a 1:1 straight linear line with the dots surrounding it
# We want the histogram of residuals to look like a normal distribution
# We want residuals vs. linear to look random – like a blood splatter lol
# For the Q-Q you want it to follow the line. Anything BELOW 0 is underestimating the error, anything ABOVE 0 is overestimating the error

#terms outside of the s() function are LINEAR terms


```

```{r Datacamp course - Hierarchical and Mixed Effects models in R}
#generally you want to use mixed-effect models for nested or repeat measures data
#nested design -- different things may effect outcomes
#random effect pools information across treatments

#summarizing data
# Summarize the student data at the classroom level
class_data <- student_data %>%
    group_by(classid, schoolid) %>%
      summarize(mathgain_class = mean(mathgain),
      mathknow_class = mean(mathknow),
      n_class = n(), .groups = "keep") #.groups ="keep" tells summarize() to keep the grouping variable

#parts of a regression:
lm(y ~ x, data=mydata)
#multiple regression includes MULTIPLE linear response variables, each with their own predictor and slope

# you can graph predictions from the model as lines
# Re-run the models to load their outputs
lm_out <- lm(mathgain ~ classid + mathkind, data = student_data)
lmer_out <- lmer(mathgain ~ mathkind + (1 | classid), data = student_data)

# Add the predictions to the original data
student_data_subset <-
    student_data %>%
    mutate(lm_predict = predict(lm_out),
           lmer_predict = predict(lmer_out)) %>%
    filter(schoolid == "1")

# Plot the predicted values
ggplot(student_data_subset,
       aes(x = mathkind, y = mathgain, color = classid)) +
    geom_point() +
    geom_line(aes(x = mathkind, y = lm_predict)) +
    geom_line(aes(x = mathkind, y = lmer_predict), linetype = 'dashed') +
    xlab("Kindergarten math score") +
    ylab("Math gain later in school") +
    theme_bw() +
    scale_color_manual("Class ID", values = c("red", "blue"))

```


#GGPLOT VISUALIZATIONS

```{r Preparing for ggplot}

#Often times before you actually plot you want to make sure your data is going to display correctly. This means you may have to subset, or re-organize a particular variable so it displays in the order that you want it to 
#Make N species dose a factor and give it levels so it will display in this order

resp_all_ammonium.data$N_species_dose <- factor(resp_all_ammonium.data$N_species_dose, levels=c("CNTRL_0", "A_5", "A_7.5","A_10"))
#now R is stupid so we have to make sure the factor has the right levels in order
env_chamber_HOBO_exp_2$date <- factor(env_chamber_HOBO_exp_2$date, levels=c("8/4/2022", "8/5/2022", "8/6/2022", "8/7/2022", "8/8/2022", "8/9/2022", "8/10/2022", "8/11/2022", "8/12/2022", "8/13/2022", "8/14/2022", "8/15/2022"))

#Make everything else a character so they graph nicely 
resp_all_ammonium.data$treat_ID_full<-as.character(resp_all_ammonium.data$treat_ID_full)
resp_all_ammonium.data$e_coli_plate<-as.character(resp_all_ammonium.data$e_coli_plate)
resp_all_ammonium.data$temp <- as.character(resp_all_ammonium.data$temp)

```

```{r ggplot - general code and various customizers}
#Before you plot in ggplot, you might want to make all of your variables factors or characters and define levels so that they display in the order (left to right) that you want. 

#Make N species dose a factor and give it levels so it will display in this order
resp_all_nitrate.data$N_species_dose <- factor(resp_all_nitrate.data$N_species_dose, levels=c("CNTRL_0", "N_1.5", "N_5","N_18"))

#Make everything else a character so they graph nicely 
resp_all_nitrate.data$treat_ID_full<-as.character(resp_all_nitrate.data$treat_ID_full)
resp_all_nitrate.data$e_coli_plate<-as.character(resp_all_nitrate.data$e_coli_plate)
resp_all_nitrate.data$temp <- as.character(resp_all_nitrate.data$temp)

#Prep the data for visualization
#This summarySE function is my BEST FRIEND. Its in the RMisc package, and it takes your data and auto-generates standard error to be used for error bars
sum_N_species_dose_nitrate <- summarySE(resp_all_nitrate.data, measurevar = "resp_rate", groupvars = c("N_species_dose"))

# Visualize
gg_N_species_dose_nitrate<-ggplot(data = sum_N_species_dose_nitrate, #the data is the dataframe it will pull from
       aes(x = N_species_dose,
           y = resp_rate, col=N_species_dose)) + #col= tells it how you want the colors to be categorized
    geom_point() + #you want points
    geom_errorbar(aes(ymin=resp_rate-se, ymax=resp_rate+se), width=0.1, size = 1) + #you want errorbars, calling from the summarySE function from Rmisc
    ggtitle ("Nitrate - N Species Dose summary") + #title for the plot
  theme(plot.title = element_text(size=12)) + #size of title
   # theme(legend.position = "right", #if you want a legend, this is how you customize it
   #        legend.title = element_text(size=12),
   #        axis.title = element_text(size = 12),
   #        legend.background = element_blank(),
   #        panel.grid.major = element_blank(),
   #        panel.grid.minor = element_blank(),
   #        axis.text = element_text(size = 12)) +
   geom_jitter(width=0.15, alpha=0.9) + #adds jitters, aka overlays raw data points and spreads them apart from each other
   theme(legend.position="none")+ #this turns off the legend
   labs(color='S initial population size') + #this changes the title of the legend
    labs(x = "\n", 
         y = expression(paste("Rate O2 Consumption (umol*min"^"-1"~"*g"^"-1"~")")), #x and y labels
         caption = "data represent mean +/- SEM" ) + #gives you a caption at the bottom right
    scale_x_discrete("Nitrate dose", labels=c(CNTRL_0="Control", N_1.5="1.5uM", N_5="5uM", N_18="18uM"))+ #relabels the x axis to the values in quotations
    scale_color_manual("Nitrate Dose", labels=c(CNTRL_0="Control", N_1.5="1.5uM", N_5="5uM", N_18="18uM"), values=c("blue", "#9DCE9D","#359C35","darkgreen")) #gives you the colors for each point on the graph
gg_N_species_dose_nitrate


# A NOTE ON GGPLOT COLORS:
  # ggplot has built in colors which can be found here: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
  # you can also add your own from a hex code using # before a unique identifier for the color. The unique identifiers can be generated here:
  # https://htmlcolorcodes.com/


```


```{r ggplots - summarize by multiple particular variables}
#You may want to visualize things binned by a couple of different independent variables
# Summarize by N_species_dose and temperature so we can visualize
#Make N species dose a factor and give it levels so it will display in this order
resp_all_ammonium.data$N_species_dose <- factor(resp_all_ammonium.data$N_species_dose, levels=c("CNTRL_0", "A_5", "A_7.5","A_10"))
#This summarySE function is my BEST FRIEND. Its in the RMisc package, and it takes your data and auto-generates standard error to be used for error bars
sum_N_species_dose_temp_ammonium <- summarySE(resp_all_ammonium.data, measurevar = "resp_rate", groupvars = c("N_species_dose","temp"))
# Visualize
gg_N_species_dose_ammonium<-ggplot(data = sum_N_species_dose_temp_ammonium,
       aes(x = N_species_dose,
           y = resp_rate, col=temp)) +
    geom_point() +
    geom_errorbar(aes(ymin=resp_rate-se, ymax=resp_rate+se), width=0.1, size = 1) +
    #scale_color_manual(values = c("black", "darkgreen")) +
    ggtitle ("Ammonium N Species Dose and Temperature Summary") +
    theme(legend.position = "right",
          legend.title = element_text(size=12),
          axis.title = element_text(size = 12),
          legend.background = element_blank(),
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(),
          axis.text = element_text(size = 12)) +
    labs(x = "\n", 
         y = expression(paste("Rate O2 Consumption (umol*min"^"-1"~"*g"^"-1"~")")), 
         caption = "data represent mean +/- SEM" ) +
    theme_classic()
gg_N_species_dose_ammonium
```

```{r ggplot - Initial vs final visualization code}
# Here is the code to make a plot that looks at two time points of the same kind of data, and includes lines with the trajectory of change in that variable

### ______________Sym starved ammonium high 20C________________________ ###
#first we need to melt the dataset by a unique identifier on the data we want to group everything by
#melt comes from the reshape2 package which you need to load
sym_starved_ammonium_high_20C_melt<- melt(sym_starved_ammonium_high_20C, id.vars="exp_1_beetag", measure.vars=c("PAM_avg_day1","PAM_avg_day11"))
#lets do a paired t test now
t.test(sym_starved_ammonium_high_20C$PAM_avg_day1,sym_starved_ammonium_high_20C$PAM_avg_day11, paired=TRUE) #p=0.01291***
#now we can plot
gg_sym_starved_ammonium_high_20C<-
  ggplot(sym_starved_ammonium_high_20C_melt, aes(x=variable, y=value, color=variable)) +
  geom_boxplot() +
  geom_jitter(width=0.15, alpha=0.9) +
  geom_line(aes(group=exp_1_beetag), col="darkgray")+
  ylab("Fv/Fm value")+
  xlab("Day of experiment")+
  theme(legend.position="none", plot.margin = unit(c(1,3, 0.5, 7),"cm"), plot.title = element_text(size = 9)) +
  #scale_color_manual(name = "Day of experiment",
                     #labels = c("Day 1","Day 11"),
                   # values = c("royalblue","darkorchid1"))+
  scale_x_discrete(labels=c("Day 1","Day 11")) +
 annotate("text", x = Inf, y = Inf, label = paste("paired t-test, p=0.01291***"), vjust = 1, hjust = 1, size=3)+
  ggtitle("Ammonium High - Sym Starved Fed 20C: Day 1 vs. Day 11 Fv/Fm")
gg_sym_starved_ammonium_high_20C
```

```{r ggplot - raw data visualization with jitters}
#Here is code to visualize boxplots with the raw data jittered over it. This can be helpful to see the spread of your data.

#sometimes you just need to make things factors before you visualize them so that they behave better in ggplot
no_e_coli$sym_state<-as.factor(no_e_coli$sym_state)
#now visualize
gg_sym_state_no_ecoli<-
  ggplot(no_e_coli, aes(x=sym_state, y=resp_rate)) +
  geom_boxplot(width=0.8, outlier.shape = NA) + #SUPER IMPORTANT!!!! Boxplot will display outliers and so will jittering, so it will double your outliers. Make sure that you use outlier.shape = NA so that the only outliers shown are from jittering
  geom_jitter(width=0.15, alpha=0.9) + #this overlays the raw data 
  ylab("Respirometry rate")+
  xlab("Treat ID")+
  theme(legend.position="none", plot.title = element_text(size = 9)) +
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+ #this makes the x axis labels up and down instead of side to side to prevent overlapping
 annotate("text", x = Inf, y = Inf, label = paste("  "), vjust = 1, hjust = 1, size=3)+
  ggtitle("Resp rate by Sym state - No E. coli")
gg_sym_state_no_ecoli
```

```{r ggplot - stacking different datasets }
gg_controls <- ggplot() + #make sure you call this empty ggplot()
  geom_boxplot(data=pam_controls_merged_RANDOM, aes(x=N_species_dose, y=PAM_delta), color="black")+
  geom_jitter(data=pam_controls_merged_RANDOM, aes(x=N_species_dose, y=PAM_delta), width=0.15, alpha=0.9, color="black") +
   geom_boxplot(data=pam_controls_ammonium.data, aes(x=N_species_dose, y=PAM_delta), color="darkgreen")+
  geom_jitter(data=pam_controls_ammonium.data, aes(x=N_species_dose, y=PAM_delta), width=0.15, alpha=0.9, color="darkgreen") +
     geom_boxplot(data=pam_controls_nitrate.data, aes(x=N_species_dose, y=PAM_delta), color="dodgerblue")+
  geom_jitter(data=pam_controls_nitrate.data, aes(x=N_species_dose, y=PAM_delta), width=0.15, alpha=0.9, color="dodgerblue") +
  ylab("Delta PAM")+
  xlab("Dose")+
  theme(legend.position="none", plot.title = element_text(size = 9)) +
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))+ #this makes the x axis labels up and down instead of side to side to prevent overlapping
 annotate("text", x = Inf, y = Inf, label = paste("  "), vjust = 1, hjust = 1, size=3)+
  ggtitle("All controls - random")
gg_controls


#stacking multiple y values from the same dataset (different variables)
#lets try those tidy skills
water_qual_graph <- water_qual_bigelow_3A %>% 
  select("treatment_ID","NO3_INTENDED", "NO3_uM") %>% 
pivot_longer(-treatment_ID, names_to = "variable", values_to = "value")

gg_intended_v_actual <- ggplot(water_qual_graph, aes(treatment_ID, value, colour = variable)) +  
   scale_color_manual(values=c("black","dodgerblue")) +
  geom_point() +
  ylab("Nitrate level (uM)")+
  xlab("Treatment ID")+
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1)) + #this makes the x axis labels up and down instead of side to side to prevent overlapping
  ylab("Nitrate level (uM)")+
  xlab("Treatment ID")+
  ggtitle("Nitrate Levels -- Intended vs. measured")
gg_intended_v_actual

```

```{r ggplot- TRENDLINES}
geom_smooth(method="lm", se=FALSE, aes(group = TREATMENT), linewidth = 0.85, linetype = "dotted")+
      stat_poly_eq(use_label(c("eq", "R2")), label.y = c(0.85, 0.9, 0.95, 1), label.x = c(0.5, 0.5, 0.5, 0.5))

#OR

  stat_smooth_func(geom="text",method="lm",hjust=0,parse=TRUE) +
  geom_smooth(formula = y ~ x, method="lm",se=FALSE) +
```

```{r Multiple axes - 2 y axes }
coeff <- 1
wq_menhaden_03ra_FLIGHTS_stock %>% ggplot(
       aes(x = as.factor(year))) +
           geom_bar(aes(y=Value), stat="identity", fill="#757575",alpha=0.8) +
        geom_bar(aes(y=Number.of.Schools), stat="identity", fill="#1E88E5", alpha=0.8) +
         scale_y_continuous(name="Number of Schools observed", sec.axis=sec_axis(~.*coeff, name="Recruitment of Age 0 fishin billions"))+ #coeff is an option to scale the other axis
    ggtitle ("Number of Schools and Recruitment of Age 0 fish (in billions)") +
  labs(x = "Year") +
  theme(
    axis.title.y = element_text(color = "#1E88E5", size=13, face="bold"),
    axis.title.y.right = element_text(color = "#757575", size=13, face="bold")
  ) 
```


```{r Model interaction plots}
#This is a bit complicated and I am just trying this out
# This is the model we will be working from
mm_resp_2 <- lmer(resp_rate ~ N_species_dose + e_coli_plate + sym_state + temp + (1|vial_ID) + (1 | col_num), data=resp_all_ammonium.data)
summary(mm_resp_2)


#Model Interaction plot - ammonium
# I for some reason cannot simplify this I dont know why
int_ammonium<-lmer(resp_rate~N_species_dose*temp*sym_state*e_coli_plate+(1|col_num), data=resp_all_ammonium.data)
summary(int_ammonium)
int_pred_ammonium <- ggemmeans(int_ammonium, terms=c("N_species_dose", "sym_state", "temp","e_coli_plate")) # I cannot do more than 4 interactions 
pred_ammonium <- ggplot(int_pred_ammonium, aes(facet,predicted, colour = group)) +
  geom_point(aes(shape = x)) +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.3) +
  labs(x = "temp", y = "Rate O2 Consumption (umol*min-1*g-1)",
       color = "temp") +
  facet_grid(~panel) +
  ggtitle("test") +
  theme_classic() +
  theme(axis.title.x = element_text(color="black", size=12),
axis.title.y = element_text(color="black", size=12))+
  theme(axis.text.x = element_text(size=12),
          axis.text.y = element_text(size=12)) +
  scale_color_manual(values=c("black", "goldenrod"))
pred_ammonium

```

#SPATIAL ANALYSES
```{r MAPS MAPS MAPS MAPS}

#install.packages("ggmap")
library(ggmap)
#install.packages('osmdata')
library(osmdata)
 
#ggmap tutorial:
# https://towardsdatascience.com/a-guide-to-using-ggmap-in-r-b283efdff2af

#here is my API key
#AIzaSyAZMk84DD46I0EMsHm0ZzSD8ATpZPQWKBo

#lets graph lisbon for funsies
lisbon_map <- get_map( getbb('lisbon'), source="stamen")
ggmap(lisbon_map)

#how to set up an API key (you need to link a credit card)
# https://www.findingyourway.io/blog/2018/12/05/2018-12-05-using-ggmap-after-july-2018/
api_secret <- 'NICE TRY' #PLEASE DO NOT USE MY API KEY, MAKE YOUR OWN!
#sounds scary but its pretty much impossible for us to reach the limit where we would be charged, each time you run get_googlemap you are charged 0.002 USD and google gives you $200 a month, so 1000 maps are $2
register_google(key = api_secret)

narra_sat <- get_map("Prudence Island, Rhode Island", maptype='satellite', source="google", api_key = api_secret, zoom=10)
ggmap(narra_sat)

astrangia_pop_locations <- data.frame(lat = c(41.717719, 41.478478), #remember you will have to rearrange here because lats need to go together and lons need to go together
                               lon = c(-71.360080, -71.354022))

providence <- data.frame(lat= 41.826901, lon= -71.413924)

labels=c("Conimicut Point","Ft. Wetherill")
ggmap(narra_sat) +
   geom_point(data = astrangia_pop_locations, aes(x = lon, y = lat), color = "orange", size = 4) +
  geom_text_repel(data=astrangia_pop_locations, aes(label = c("Conimicut Point","Ft. Wetherill")), color="white")+
  geom_point(data=providence, aes(x = lon, y = lat), color = "blue", size = 5) +  geom_text_repel(data=providence, aes(label = "Providence"), color="white")+
  ggtitle("Astrangia Populations in Narragansett Bay")

```



